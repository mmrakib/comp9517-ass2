{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/Cadene/pretrained-models.pytorch\n",
    "\n",
    "import pretrainedmodels\n",
    "import torch\n",
    "from Dataloader import load_and_preprocess_dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1508 295 106 715\n",
      "----- Method:[reduce_dataset], ran in 0.16181230545043945 Seconds,\n",
      "588 117 56 313\n",
      "----- Method:[remove_cell_wires], ran in 0.4165050983428955 Seconds,\n",
      "----- Method:[split_t_t_data], ran in 0.3147096633911133 Seconds,\n",
      "441 88 42 234\n",
      "147 29 14 79\n",
      "----- Method:[expand_dataset], ran in 0.5301566123962402 Seconds,\n",
      "----- Method:[expand_dataset], ran in 0.16232037544250488 Seconds,\n",
      "----- Method:[shuffle_set], ran in 0.3378615379333496 Seconds,\n",
      "1764 352 168 936\n",
      "588 116 56 316\n",
      "----- Method:[make_3_channel], ran in 2.581594705581665 Seconds,\n",
      "----- Method:[make_3_channel], ran in 1.7840611934661865 Seconds,\n",
      "----- Method:[load_and_preprocess_dataset], ran in 29.090898513793945 Seconds,\n"
     ]
    }
   ],
   "source": [
    "train_imgs, train_probs, train_types, test_imgs, test_probs, test_types = load_and_preprocess_dataset(wire_removal=\"Gray\", augment=\"All\", out_types=\"Mono\", aug_types = [\"Flip\", \"Rot\"], channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (_features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (linear0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (relu0): ReLU(inplace=True)\n",
       "  (dropout0): Dropout(p=0.5, inplace=False)\n",
       "  (linear1): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (relu1): ReLU(inplace=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (last_linear): Linear(in_features=4096, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = pretrainedmodels.vgg19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in base_model._features:\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.linear0.requires_grad = True\n",
    "base_model.relu0.requires_grad = True\n",
    "base_model.dropout0.requires_grad = True\n",
    "base_model.linear1.requires_grad = True\n",
    "base_model.relu1.requires_grad = True\n",
    "base_model.dropout1.requires_grad = True\n",
    "base_model.last_linear.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning\n",
    "dim_feats = base_model.last_linear.in_features\n",
    "nb_classes = 4\n",
    "base_model.last_linear = torch.nn.Linear(dim_feats, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(base_model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = AllDataset(train_imgs, train_probs, transform)\n",
    "validation_split = 0.2\n",
    "dataset_size = len(dataset)\n",
    "val_size = int(validation_split * dataset_size)\n",
    "train_size = dataset_size - val_size\n",
    "\n",
    "# Use random_split to create training and validation datasets\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    base_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_images, batch_labels in train_dataloader:\n",
    "        \n",
    "        batch_labels = (batch_labels * (4 - 1)).round().long()\n",
    "        one_hot_labels = torch.zeros(len(batch_labels), 4)\n",
    "        one_hot_labels.scatter_(1, batch_labels.view(-1, 1), 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = base_model(batch_images)\n",
    "        loss = criterion(outputs, one_hot_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # print(f'Batch Loss: {loss.item():.4f}')\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_samples += one_hot_labels.size(0)\n",
    "        correct_predictions += (predicted == batch_labels).sum().item()\n",
    "\n",
    "    average_loss = running_loss / len(train_dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    print(f'Training Epoch [{epoch + 1}/{epochs}], Loss: {average_loss:.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    base_model.eval()  # Set the model to evaluation mode\n",
    "    val_running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels in val_dataloader:\n",
    "            \n",
    "            val_outputs = base_model(val_images)\n",
    "            \n",
    "            val_labels = (val_labels * (4 - 1)).round().long()\n",
    "            one_hot_labels = torch.zeros(len(val_labels), 4)\n",
    "            one_hot_labels.scatter_(1, val_labels.view(-1, 1), 1)\n",
    "            val_loss = criterion(val_outputs, one_hot_labels)\n",
    "\n",
    "            val_running_loss += val_loss.item()\n",
    "\n",
    "            _, predicted = torch.max(val_outputs, 1)\n",
    "            total_samples += val_labels.size(0)\n",
    "            correct_predictions += (predicted == val_labels).sum().item()\n",
    "\n",
    "    val_average_loss = val_running_loss / len(val_dataloader)\n",
    "    val_accuracy = correct_predictions / total_samples\n",
    "\n",
    "    print(f'Validation Epoch [{epoch + 1}/{epochs}], Validation Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5464684014869888\n"
     ]
    }
   ],
   "source": [
    "test_dataset = AllDataset(test_imgs, test_probs, transform)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "base_model.eval()\n",
    "\n",
    "# Initialize variables for accuracy calculation\n",
    "running_loss = 0.0\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "all_predictions = torch.Tensor([])\n",
    "all_labels = torch.Tensor([])\n",
    "\n",
    "# Disable gradient calculation during testing\n",
    "with torch.no_grad():\n",
    "\n",
    "    for test_images, test_labels in test_dataloader:\n",
    "\n",
    "        outputs = base_model(test_images)\n",
    "        \n",
    "        test_labels = (test_labels * (4 - 1)).round().long()\n",
    "        one_hot_labels = torch.zeros(len(test_labels), 4)\n",
    "        one_hot_labels.scatter_(1, test_labels.view(-1, 1), 1)\n",
    "\n",
    "        loss = criterion(outputs, one_hot_labels)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_samples += test_labels.size(0)\n",
    "        correct_predictions += (predicted == test_labels).sum().item()\n",
    "        all_predictions = torch.cat((all_predictions, predicted))\n",
    "        all_labels = torch.cat((all_labels, test_labels))\n",
    "\n",
    "    average_loss = running_loss / len(test_dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy = correct_predictions / total_samples\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.55      0.71      1076\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "         2.0       0.00      0.00      0.00         0\n",
      "         3.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.55      1076\n",
      "   macro avg       0.25      0.14      0.18      1076\n",
      "weighted avg       1.00      0.55      0.71      1076\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(all_predictions, all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(base_model, '../models/vgg19-bw.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
